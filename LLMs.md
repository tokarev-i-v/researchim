Статья «noise_step: Training in 1.58b With No Gradient Memory» (код и статья: https://github.com/wbrickner/noise_step)
представляет новый алгоритм, предназначенный для эффективной тренировки больших моделей машинного обучения в условиях крайне низкой точности (1.58 бита, или тернарная точность). Вот основные аспекты статьи с деталями:

---

### **Основная идея**
- **Проблема**: Тренировка больших моделей требует огромных вычислительных ресурсов и энергии. Несмотря на успехи в снижении точности во время инференса (1.58-битная точность без потерь), процесс тренировки остается ресурсоемким, обычно выполняясь в f16.
- **Решение**: Новый алгоритм работает с тернарными представлениями весов (значения {-1, 0, +1}) и может обучать модель без использования градиентного спуска, что значительно снижает требования к памяти и энергии.

---

### **Градиентное оценивание (Gradient Estimation)**

1. **Методика вычисления градиента**:
   - Используется якобиан-векторное произведение \( J_f \nu \), чтобы оценить направление и величину градиента.
   - Для оценки градиента в тернарном пространстве вводятся разреженные шумовые векторы \( \nu_i \), где \( \nu_i \sim \text{Bernoulli}(s) \odot \text{U}\{-1, +1\} \).

2. **Алгоритм оценки**:
   - Градиент оценивается как взвешенная сумма направлений шумов.
   - Только знак выравнивания (alignment) используется для вычисления, что упрощает расчеты:
     \[
     \hat{\nabla}f = \sum \nu_i \cdot \text{sgn}(\alpha(\nu_i)).
     \]
   - Для улучшения сходимости отклоняются шумовые векторы с малыми значениями выравнивания (используется медиана).

3. **Гиперпараметры**:
   - Единственные параметры — количество и разреженность шумовых векторов.

---

### **Эффективность представления (Representation Efficiency)**

1. **Генерация шумов**:
   - Шумовые векторы \( \nu_i \) не нужно хранить в памяти или передавать. Они генерируются с использованием псевдослучайного генератора, что минимизирует затраты памяти.

2. **Распределенная тренировка**:
   - Алгоритм значительно снижает объем данных, которые нужно передавать между устройствами. Градиенты кодируются всего 1.58 битами на одну возмущение.

3. **Транспортировка модели**:
   - Модель может быть представлена через последовательность шагов. Например, тернарная версия GPT-3 (175B параметров) может быть сжата до 600 КБ – 19 МБ.

---

### **Свойства сходимости**

1. **Шум в градиентах**:
   - Из-за дискретной природы тернарного пространства траектории весов становятся прерывистыми, а функции потерь более шумными.
   - Для стабильной оптимизации требуется использование больших размеров батчей.

2. **Эмпирическая проверка**:
   - Было проведено сравнение алгоритма на задаче классификации MNIST между noise_step и Adam. Noise_step показал близкую эффективность, но с несколько худшими характеристиками на высоких шагах.

---

### **Рекомендации по реализации**

1. **Эффективное кодирование**:
   - Для хранения тернарных значений предлагается использовать специальные кодировки, минимизирующие затраты памяти.

2. **Разреженность шума**:
   - Высокая разреженность шума позволяет упрощать вычисления, сохраняя эффективность.

3. **Ортогональность шума**:
   - Ортогональные возмущения обеспечивают максимум информации о градиенте, что улучшает сходимость.

---

### **Преимущества и ограничения**

1. **Преимущества**:
   - Значительное сокращение памяти и энергии.
   - Возможность распределенной тренировки с низкими затратами на передачу данных.
   - Универсальность: модель может быть восстановлена из последовательности шагов в любой момент.

2. **Ограничения**:
   - Полная реконструкция модели требует значительных вычислений.
   - Тренировка чувствительна к выбору плотности шума, что требует дополнительной настройки.

---

Если вам нужны уточнения или подробности по конкретному разделу, сообщите!
